{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Titanic Analysis","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents","metadata":{}},{"cell_type":"markdown","source":"Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancÃ©s were ignored)\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-06T23:11:13.557872Z","iopub.execute_input":"2024-11-06T23:11:13.558305Z","iopub.status.idle":"2024-11-06T23:11:15.381041Z","shell.execute_reply.started":"2024-11-06T23:11:13.558261Z","shell.execute_reply":"2024-11-06T23:11:15.379466Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.383313Z","iopub.execute_input":"2024-11-06T23:11:15.384015Z","iopub.status.idle":"2024-11-06T23:11:15.442685Z","shell.execute_reply.started":"2024-11-06T23:11:15.383948Z","shell.execute_reply":"2024-11-06T23:11:15.441213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.444502Z","iopub.execute_input":"2024-11-06T23:11:15.444999Z","iopub.status.idle":"2024-11-06T23:11:15.488043Z","shell.execute_reply.started":"2024-11-06T23:11:15.444943Z","shell.execute_reply":"2024-11-06T23:11:15.486551Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Sex'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.491147Z","iopub.execute_input":"2024-11-06T23:11:15.491648Z","iopub.status.idle":"2024-11-06T23:11:15.508130Z","shell.execute_reply.started":"2024-11-06T23:11:15.491578Z","shell.execute_reply":"2024-11-06T23:11:15.506537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Ticket'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.510106Z","iopub.execute_input":"2024-11-06T23:11:15.510707Z","iopub.status.idle":"2024-11-06T23:11:15.528327Z","shell.execute_reply.started":"2024-11-06T23:11:15.510646Z","shell.execute_reply":"2024-11-06T23:11:15.526709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Cabin'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.530268Z","iopub.execute_input":"2024-11-06T23:11:15.530803Z","iopub.status.idle":"2024-11-06T23:11:15.553223Z","shell.execute_reply.started":"2024-11-06T23:11:15.530732Z","shell.execute_reply":"2024-11-06T23:11:15.551763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.555267Z","iopub.execute_input":"2024-11-06T23:11:15.555830Z","iopub.status.idle":"2024-11-06T23:11:15.579092Z","shell.execute_reply.started":"2024-11-06T23:11:15.555769Z","shell.execute_reply":"2024-11-06T23:11:15.577809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Embarked'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.580714Z","iopub.execute_input":"2024-11-06T23:11:15.581064Z","iopub.status.idle":"2024-11-06T23:11:15.600597Z","shell.execute_reply.started":"2024-11-06T23:11:15.581027Z","shell.execute_reply":"2024-11-06T23:11:15.599189Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Survived'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.602913Z","iopub.execute_input":"2024-11-06T23:11:15.603416Z","iopub.status.idle":"2024-11-06T23:11:15.624125Z","shell.execute_reply.started":"2024-11-06T23:11:15.603341Z","shell.execute_reply":"2024-11-06T23:11:15.622770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.630217Z","iopub.execute_input":"2024-11-06T23:11:15.630682Z","iopub.status.idle":"2024-11-06T23:11:15.677193Z","shell.execute_reply.started":"2024-11-06T23:11:15.630634Z","shell.execute_reply":"2024-11-06T23:11:15.674816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.hist(bins = 50,figsize=(20,15))","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:15.679291Z","iopub.execute_input":"2024-11-06T23:11:15.679846Z","iopub.status.idle":"2024-11-06T23:11:18.641954Z","shell.execute_reply.started":"2024-11-06T23:11:15.679778Z","shell.execute_reply":"2024-11-06T23:11:18.640426Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count of NaNs in each column\nnan_counts = df.isna().sum()\nprint(nan_counts)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.643878Z","iopub.execute_input":"2024-11-06T23:11:18.644338Z","iopub.status.idle":"2024-11-06T23:11:18.656179Z","shell.execute_reply.started":"2024-11-06T23:11:18.644287Z","shell.execute_reply":"2024-11-06T23:11:18.654793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Age'] = df['Age'].fillna(df['Age'].median())\n# Fill NaNs in Embarked with the mode\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.657837Z","iopub.execute_input":"2024-11-06T23:11:18.658257Z","iopub.status.idle":"2024-11-06T23:11:18.675937Z","shell.execute_reply.started":"2024-11-06T23:11:18.658214Z","shell.execute_reply":"2024-11-06T23:11:18.674763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_Validation = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nX_Validation['Age'] = X_Validation['Age'].fillna(X_Validation['Age'].median())\nX_Validation['Fare'] = X_Validation['Fare'].fillna(X_Validation['Fare'].median())\nnan_counts = X_Validation.isna().sum()\nprint(nan_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.677547Z","iopub.execute_input":"2024-11-06T23:11:18.678324Z","iopub.status.idle":"2024-11-06T23:11:18.705537Z","shell.execute_reply.started":"2024-11-06T23:11:18.678258Z","shell.execute_reply":"2024-11-06T23:11:18.704015Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_Validation","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.707094Z","iopub.execute_input":"2024-11-06T23:11:18.707821Z","iopub.status.idle":"2024-11-06T23:11:18.739980Z","shell.execute_reply.started":"2024-11-06T23:11:18.707776Z","shell.execute_reply":"2024-11-06T23:11:18.738425Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nan_counts = df.isna().sum()\nprint(nan_counts)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.741819Z","iopub.execute_input":"2024-11-06T23:11:18.742322Z","iopub.status.idle":"2024-11-06T23:11:18.752858Z","shell.execute_reply.started":"2024-11-06T23:11:18.742267Z","shell.execute_reply":"2024-11-06T23:11:18.751595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.754392Z","iopub.execute_input":"2024-11-06T23:11:18.754849Z","iopub.status.idle":"2024-11-06T23:11:18.780449Z","shell.execute_reply.started":"2024-11-06T23:11:18.754807Z","shell.execute_reply":"2024-11-06T23:11:18.778868Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Define the target and features\nX = df.drop(\"Survived\", axis=1)\ny = df[\"Survived\"]\n\n# Create Age bins for analysis\nage_bins = [0, 12, 18, 35, 60, 80]\nage_labels = ['Child', 'Teen', 'Young Adult', 'Adult', 'Senior']\ndf['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels)\n\n# Create Fare bins for analysis\nfare_bins = [0, 10, 50, 100, 600]\nfare_labels = ['Low', 'Medium', 'High', 'Very High']\ndf['FareGroup'] = pd.cut(df['Fare'], bins=fare_bins, labels=fare_labels)\n\n# Define the StratifiedShuffleSplit\nstratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n\n# Perform the split\nfor train_index, test_index in stratified_split.split(X, y):\n    train_set, test_set = df.iloc[train_index], df.iloc[test_index]\n\n# Define a function to calculate distributions\ndef calculate_distribution(data, column):\n    return data[column].value_counts(normalize=True) * 100\n\n# Initialize the distribution dictionary\ndistribution_dict = {}\n\n# Add each feature to the dictionary\nfeatures = {\n    \"Survived\": df[\"Survived\"].unique(),\n    \"AgeGroup\": age_labels,\n    \"FareGroup\": fare_labels,\n    \"Pclass\": df[\"Pclass\"].unique(),\n    \"Sex\": df[\"Sex\"].unique(),\n    \"SibSp\": df[\"SibSp\"].unique(),\n    \"Parch\": df[\"Parch\"].unique(),\n    \"Embarked\": df[\"Embarked\"].unique()\n}\n\n# Populate distribution dictionary with all features\nfor feature, categories in features.items():\n    for category in categories:\n        distribution_dict[(feature, category)] = {\n            \"Original\": calculate_distribution(df, feature).get(category, 0),\n            \"Train\": calculate_distribution(train_set, feature).get(category, 0),\n            \"Test\": calculate_distribution(test_set, feature).get(category, 0)\n        }\n\n# Convert the dictionary to a DataFrame\ndistribution_df = pd.DataFrame(distribution_dict).T\ndistribution_df.index.names = ['Feature', 'Category']\ndistribution_df = distribution_df.fillna(0)\n\n# Display the DataFrame\ndistribution_df\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:18.783376Z","iopub.execute_input":"2024-11-06T23:11:18.783909Z","iopub.status.idle":"2024-11-06T23:11:19.745537Z","shell.execute_reply.started":"2024-11-06T23:11:18.783853Z","shell.execute_reply":"2024-11-06T23:11:19.744271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:19.747126Z","iopub.execute_input":"2024-11-06T23:11:19.747684Z","iopub.status.idle":"2024-11-06T23:11:19.774039Z","shell.execute_reply.started":"2024-11-06T23:11:19.747640Z","shell.execute_reply":"2024-11-06T23:11:19.772749Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:19.776013Z","iopub.execute_input":"2024-11-06T23:11:19.777295Z","iopub.status.idle":"2024-11-06T23:11:19.800717Z","shell.execute_reply.started":"2024-11-06T23:11:19.777227Z","shell.execute_reply":"2024-11-06T23:11:19.799329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.scatterplot(data = df, x = 'Age', y = 'Fare')","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:19.802189Z","iopub.execute_input":"2024-11-06T23:11:19.802643Z","iopub.status.idle":"2024-11-06T23:11:20.594390Z","shell.execute_reply.started":"2024-11-06T23:11:19.802563Z","shell.execute_reply":"2024-11-06T23:11:20.592619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Set up the plotting area\nplt.figure(figsize=(15, 10))\n\n# Plotting Pclass\nplt.subplot(2, 4, 1)\nsns.countplot(data=df, x='Pclass')\nplt.title('Passenger Class Distribution')\n\n# Plotting Sex\nplt.subplot(2, 4, 2)\nsns.countplot(data=df, x='Sex')\nplt.title('Sex Distribution')\n\n# Plotting Age\nplt.subplot(2, 4, 3)\nsns.histplot(data=df, x='Age', bins=30, kde=True)\nplt.title('Age Distribution')\n\n# Plotting SibSp\nplt.subplot(2, 4, 4)\nsns.countplot(data=df, x='SibSp')\nplt.title('Number of Siblings/Spouses')\n\n# Plotting Parch\nplt.subplot(2, 4, 5)\nsns.countplot(data=df, x='Parch')\nplt.title('Number of Parents/Children')\n\n# Plotting Fare\nplt.subplot(2, 4, 6)\nsns.histplot(data=df, x='Fare', bins=30, kde=True)\nplt.title('Fare Distribution')\n\n# Plotting Embarked\nplt.subplot(2, 4, 7)\nsns.countplot(data=df, x='Embarked')\nplt.title('Embarked Distribution')\n\n# Plotting AgeGroup\nplt.subplot(2, 4, 8)\nsns.countplot(data=df, x='AgeGroup')\nplt.title('Age Group Distribution')\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:20.596300Z","iopub.execute_input":"2024-11-06T23:11:20.597172Z","iopub.status.idle":"2024-11-06T23:11:22.915756Z","shell.execute_reply.started":"2024-11-06T23:11:20.597098Z","shell.execute_reply":"2024-11-06T23:11:22.914391Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Set up the plotting area\nplt.figure(figsize=(15, 10))\n\n# Plotting Pclass vs Survived\nplt.subplot(2, 4, 1)\nsns.countplot(data=df, x='Pclass', hue='Survived')\nplt.title('Survival by Passenger Class')\nplt.legend(title='Survived', labels=['No', 'Yes'])\n\n# Plotting Sex vs Survived\nplt.subplot(2, 4, 2)\nsns.countplot(data=df, x='Sex', hue='Survived')\nplt.title('Survival by Sex')\nplt.legend(title='Survived', labels=['No', 'Yes'])\n\n# Plotting Age vs Survived\nplt.subplot(2, 4, 3)\nsns.boxplot(data=df, x='Survived', y='Age')\nplt.title('Age Distribution by Survival')\nplt.xticks([0, 1], ['No', 'Yes'])\n\n# Plotting SibSp vs Survived\nplt.subplot(2, 4, 4)\nsns.countplot(data=df, x='SibSp', hue='Survived')\nplt.title('Survival by Siblings/Spouses')\nplt.legend(title='Survived', labels=['No', 'Yes'])\n\n# Plotting Parch vs Survived\nplt.subplot(2, 4, 5)\nsns.countplot(data=df, x='Parch', hue='Survived')\nplt.title('Survival by Parents/Children')\nplt.legend(title='Survived', labels=['No', 'Yes'])\n\n# Plotting Fare vs Survived\nplt.subplot(2, 4, 6)\nsns.boxplot(data=df, x='Survived', y='Fare')\nplt.title('Fare Distribution by Survival')\nplt.xticks([0, 1], ['No', 'Yes'])\n\n# Plotting Embarked vs Survived\nplt.subplot(2, 4, 7)\nsns.countplot(data=df, x='Embarked', hue='Survived')\nplt.title('Survival by Embarked')\nplt.legend(title='Survived', labels=['No', 'Yes'])\n\n# Plotting AgeGroup vs Survived\nplt.subplot(2, 4, 8)\nsns.countplot(data=df, x='AgeGroup', hue='Survived')\nplt.title('Survival by Age Group')\nplt.legend(title='Survived', labels=['No', 'Yes'])\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:22.917464Z","iopub.execute_input":"2024-11-06T23:11:22.917934Z","iopub.status.idle":"2024-11-06T23:11:25.180277Z","shell.execute_reply.started":"2024-11-06T23:11:22.917882Z","shell.execute_reply":"2024-11-06T23:11:25.178916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Set up the plotting area\nplt.figure(figsize=(15, 10))\n\n# Scatter plot for Age vs Survived\nplt.subplot(2, 3, 1)\nsns.scatterplot(data=df, x='Age', y='Survived', alpha=0.6)\nplt.title('Age vs Survival')\nplt.xlabel('Age')\nplt.ylabel('Survived (0 = No, 1 = Yes)')\n\n# Scatter plot for Fare vs Survived\nplt.subplot(2, 3, 2)\nsns.scatterplot(data=df, x='Fare', y='Survived', alpha=0.6)\nplt.title('Fare vs Survival')\nplt.xlabel('Fare')\nplt.ylabel('Survived (0 = No, 1 = Yes)')\n\n# Scatter plot for SibSp vs Survived\nplt.subplot(2, 3, 3)\nsns.scatterplot(data=df, x='SibSp', y='Survived', alpha=0.6)\nplt.title('Siblings/Spouses vs Survival')\nplt.xlabel('Siblings/Spouses')\nplt.ylabel('Survived (0 = No, 1 = Yes)')\n\n# Scatter plot for Parch vs Survived\nplt.subplot(2, 3, 4)\nsns.scatterplot(data=df, x='Parch', y='Survived', alpha=0.6)\nplt.title('Parents/Children vs Survival')\nplt.xlabel('Parents/Children')\nplt.ylabel('Survived (0 = No, 1 = Yes)')\n\n# Scatter plot for Pclass vs Survived\nplt.subplot(2, 3, 5)\nsns.scatterplot(data=df, x='Pclass', y='Survived', alpha=0.6)\nplt.title('Passenger Class vs Survival')\nplt.xlabel('Passenger Class')\nplt.ylabel('Survived (0 = No, 1 = Yes)')\n\n# Scatter plot for Sex vs Survived (Note: Sex is numeric after encoding)\nplt.subplot(2, 3, 6)\nsns.scatterplot(data=df, x='Sex', y='Survived', alpha=0.6)\nplt.title('Sex vs Survival')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.ylabel('Survived (0 = No, 1 = Yes)')\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:25.182388Z","iopub.execute_input":"2024-11-06T23:11:25.182863Z","iopub.status.idle":"2024-11-06T23:11:27.051979Z","shell.execute_reply.started":"2024-11-06T23:11:25.182812Z","shell.execute_reply":"2024-11-06T23:11:27.050694Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding for categorical variables\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Drop non-numeric columns, including 'Name' if it exists\ndf_numeric = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Name'], errors='ignore')\n\n# Calculate the correlation matrix\ncorrelation_matrix = df_numeric.corr()\n\n# Set up the plotting area\nplt.figure(figsize=(12, 10))\n\n# Create a heatmap\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar=True,\n            xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)\n\nplt.title('Correlation Matrix of Titanic Dataset')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:27.053920Z","iopub.execute_input":"2024-11-06T23:11:27.054442Z","iopub.status.idle":"2024-11-06T23:11:27.808691Z","shell.execute_reply.started":"2024-11-06T23:11:27.054380Z","shell.execute_reply":"2024-11-06T23:11:27.807446Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Pclass_Fare_Interaction'] = df['Pclass'] * df['Fare']","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:27.810516Z","iopub.execute_input":"2024-11-06T23:11:27.810890Z","iopub.status.idle":"2024-11-06T23:11:27.818395Z","shell.execute_reply.started":"2024-11-06T23:11:27.810851Z","shell.execute_reply":"2024-11-06T23:11:27.816941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding for categorical variables\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Drop non-numeric columns, including 'Name' if it exists\ndf_numeric = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Name'], errors='ignore')\n\n# Calculate the correlation matrix\ncorrelation_matrix = df_numeric.corr()\n\n# Set up the plotting area\nplt.figure(figsize=(12, 10))\n\n# Create a heatmap\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar=True,\n            xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)\n\nplt.title('Correlation Matrix of Titanic Dataset')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:27.820334Z","iopub.execute_input":"2024-11-06T23:11:27.820866Z","iopub.status.idle":"2024-11-06T23:11:28.720254Z","shell.execute_reply.started":"2024-11-06T23:11:27.820809Z","shell.execute_reply":"2024-11-06T23:11:28.719048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Fare_SibSp_Interaction'] = df['Fare'] * df['SibSp']\ndf['Fare_Parch_Interaction'] = df['Fare'] * df['Parch']\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1  # Adding 1 for the passenger themselves\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:28.732433Z","iopub.execute_input":"2024-11-06T23:11:28.732891Z","iopub.status.idle":"2024-11-06T23:11:28.743950Z","shell.execute_reply.started":"2024-11-06T23:11:28.732847Z","shell.execute_reply":"2024-11-06T23:11:28.742531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding for categorical variables\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Drop non-numeric columns, including 'Name' if it exists\ndf_numeric = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Name'], errors='ignore')\n\n# Calculate the correlation matrix\ncorrelation_matrix = df_numeric.corr()\n\n# Set up the plotting area\nplt.figure(figsize=(12, 10))\n\n# Create a heatmap\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar=True,\n            xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)\n\nplt.title('Correlation Matrix of Titanic Dataset')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:28.745649Z","iopub.execute_input":"2024-11-06T23:11:28.746014Z","iopub.status.idle":"2024-11-06T23:11:29.860849Z","shell.execute_reply.started":"2024-11-06T23:11:28.745969Z","shell.execute_reply":"2024-11-06T23:11:29.859465Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['AgeGroup_Parch_Interaction'] = df['AgeGroup'].astype(str) + '_' + df['Parch'].astype(str)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:29.862265Z","iopub.execute_input":"2024-11-06T23:11:29.862704Z","iopub.status.idle":"2024-11-06T23:11:29.873442Z","shell.execute_reply.started":"2024-11-06T23:11:29.862661Z","shell.execute_reply":"2024-11-06T23:11:29.871775Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your Titanic dataset\n# df = pd.read_csv(\"path/to/titanic.csv\")  # Uncomment and specify your path\n\n# Label Encoding for categorical variables\nlabel_encoders = {}\nfor column in ['Sex', 'Embarked', 'AgeGroup', 'FareGroup']:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column].astype(str))  # Convert to string to avoid errors\n    label_encoders[column] = le  # Store the encoder for potential inverse transformation later\n\n# Drop non-numeric columns, including 'Name' if it exists\ndf_numeric = df.select_dtypes(include=['float64', 'int64']).drop(columns=['Name'], errors='ignore')\n\n# Calculate the correlation matrix\ncorrelation_matrix = df_numeric.corr()\n\n# Set up the plotting area\nplt.figure(figsize=(12, 10))\n\n# Create a heatmap\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar=True,\n            xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)\n\nplt.title('Correlation Matrix of Titanic Dataset')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:29.875129Z","iopub.execute_input":"2024-11-06T23:11:29.875999Z","iopub.status.idle":"2024-11-06T23:11:31.281642Z","shell.execute_reply.started":"2024-11-06T23:11:29.875952Z","shell.execute_reply":"2024-11-06T23:11:31.280408Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.283325Z","iopub.execute_input":"2024-11-06T23:11:31.283813Z","iopub.status.idle":"2024-11-06T23:11:31.311007Z","shell.execute_reply.started":"2024-11-06T23:11:31.283758Z","shell.execute_reply":"2024-11-06T23:11:31.309463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = ['Pclass', 'Sex','Age','Fare','Embarked']\ntype(features)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.312435Z","iopub.execute_input":"2024-11-06T23:11:31.312835Z","iopub.status.idle":"2024-11-06T23:11:31.324813Z","shell.execute_reply.started":"2024-11-06T23:11:31.312794Z","shell.execute_reply":"2024-11-06T23:11:31.323457Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set_X=train_set[features]\ntrain_set_y = train_set['Survived']\ntrain_set_X.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.326420Z","iopub.execute_input":"2024-11-06T23:11:31.327185Z","iopub.status.idle":"2024-11-06T23:11:31.348617Z","shell.execute_reply.started":"2024-11-06T23:11:31.327123Z","shell.execute_reply":"2024-11-06T23:11:31.347383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_set.head()\n\ntest_set_X=test_set[features]\ntest_set_y = test_set['Survived']\ntest_set_X.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.349959Z","iopub.execute_input":"2024-11-06T23:11:31.350328Z","iopub.status.idle":"2024-11-06T23:11:31.370914Z","shell.execute_reply.started":"2024-11-06T23:11:31.350290Z","shell.execute_reply":"2024-11-06T23:11:31.369704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_Validation_passenger_ids = X_Validation['PassengerId'].copy()\nX_Validation = X_Validation[features]\nX_Validation.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.372416Z","iopub.execute_input":"2024-11-06T23:11:31.372798Z","iopub.status.idle":"2024-11-06T23:11:31.391894Z","shell.execute_reply.started":"2024-11-06T23:11:31.372759Z","shell.execute_reply":"2024-11-06T23:11:31.390428Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_Validation_passenger_ids","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.393328Z","iopub.execute_input":"2024-11-06T23:11:31.393750Z","iopub.status.idle":"2024-11-06T23:11:31.408250Z","shell.execute_reply.started":"2024-11-06T23:11:31.393709Z","shell.execute_reply":"2024-11-06T23:11:31.407046Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming train_set_X, train_set_y, test_set_X, and test_set_y are defined\n# Step 1: Get dummy variables for categorical features\ntrain_set_X = pd.get_dummies(train_set_X, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\ntest_set_X = pd.get_dummies(test_set_X, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\n\n# Step 2: Standardize numerical features\nscaler = StandardScaler()\ntrain_set_X[['Fare', 'Age']] = scaler.fit_transform(train_set_X[['Fare', 'Age']])\ntest_set_X[['Fare', 'Age']] = scaler.transform(test_set_X[['Fare', 'Age']])\n\n# Step 3: Train Random Forest Regressor\nrf_model = RandomForestRegressor(random_state=42)\nrf_model.fit(train_set_X, train_set_y)\n\n# Step 4: Make predictions (optional)\npredictions = rf_model.predict(test_set_X)\n\n# Output model score\nprint(\"Training Score:\", rf_model.score(train_set_X, train_set_y))\nprint(\"Test Score:\", rf_model.score(test_set_X, test_set_y))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.409811Z","iopub.execute_input":"2024-11-06T23:11:31.410259Z","iopub.status.idle":"2024-11-06T23:11:31.422604Z","shell.execute_reply.started":"2024-11-06T23:11:31.410204Z","shell.execute_reply":"2024-11-06T23:11:31.421348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\n\n# Assuming train_set_X, train_set_y, test_set_X, test_set_y, and X_Validation are already defined\n# and your datasets are already split\n\n\n\n# Step 1: Get dummy variables for categorical features\ntrain_set_X = pd.get_dummies(train_set_X, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\ntest_set_X = pd.get_dummies(test_set_X, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\nX_Validation = pd.get_dummies(X_Validation, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\n\n# Ensure both train, test, and validation sets have the same dummy variables\ntrain_set_X, test_set_X = train_set_X.align(test_set_X, join='inner', axis=1)\ntrain_set_X, X_Validation = train_set_X.align(X_Validation, join='inner', axis=1)\n\n# Step 2: Standardize numerical features\nscaler = StandardScaler()\ntrain_set_X[['Fare', 'Age']] = scaler.fit_transform(train_set_X[['Fare', 'Age']])\ntest_set_X[['Fare', 'Age']] = scaler.transform(test_set_X[['Fare', 'Age']])\nX_Validation[['Fare', 'Age']] = scaler.transform(X_Validation[['Fare', 'Age']])\n\n# List of models to try\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"Support Vector Machine\": SVC(),\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n}\n\n# Function to create a simple neural network model using TensorFlow\ndef create_nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n\n\n\n# Adding neural network to the models dictionary\ninput_dim = train_set_X.shape[1]\nnn_model = create_nn_model(input_dim)\nmodels[\"Neural Network\"] = nn_model\n\n# Train and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\nbest_model = None\n\nfor model_name, model in models.items():\n    if model_name == \"Neural Network\":\n        model.fit(train_set_X, train_set_y, epochs=100, batch_size=32, verbose=0)\n        train_predictions = (model.predict(train_set_X) > 0.5).astype(\"int32\")\n        test_predictions = (model.predict(test_set_X) > 0.5).astype(\"int32\")\n        validation_predictions = (model.predict(X_Validation) > 0.5).astype(\"int32\")\n    else:\n        model.fit(train_set_X, train_set_y)\n        train_predictions = model.predict(train_set_X)\n        test_predictions = model.predict(test_set_X)\n        validation_predictions = model.predict(X_Validation)\n    \n    train_accuracy = accuracy_score(train_set_y, train_predictions)\n    test_accuracy = accuracy_score(test_set_y, test_predictions)\n    \n    print(f\"{model_name} Training Accuracy: {train_accuracy}\")\n    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n    \n    if test_accuracy > best_accuracy:\n        best_accuracy = test_accuracy\n        best_model_name = model_name\n        best_model = model\n\n    # Create a DataFrame for the model's submission\n    submission = pd.DataFrame({\n        'PassengerId': X_Validation_passenger_ids,  # Use the stored PassengerId\n        'Survived': validation_predictions.flatten()  # Flatten in case of neural network output\n    })\n\n    # Save to a CSV file for each model\n    submission_file = f'{model_name}_submission.csv'\n    submission.to_csv(submission_file, index=False)\n    print(f\"Submission file created: {submission_file}\")\n\nprint(f\"Best Model: {best_model_name} with test accuracy {best_accuracy}\")\n\n# Best model's predictions on the validation set\nif best_model_name == \"Neural Network\":\n    final_validation_predictions = (best_model.predict(X_Validation) > 0.5).astype(\"int32\")\nelse:\n    final_validation_predictions = best_model.predict(X_Validation)\n\n# Create a DataFrame for the best model's final submission\nfinal_submission = pd.DataFrame({\n    'PassengerId': X_Validation_passenger_ids,  # Use the stored PassengerId\n    'Survived': final_validation_predictions.flatten()  # Flatten in case of neural network output\n})\n\n# Save the final submission to submission.csv\nfinal_submission.to_csv('submission.csv', index=False)\nprint(\"Final submission file created: submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-06T23:11:31.424623Z","iopub.execute_input":"2024-11-06T23:11:31.425020Z","iopub.status.idle":"2024-11-06T23:12:00.092407Z","shell.execute_reply.started":"2024-11-06T23:11:31.424980Z","shell.execute_reply":"2024-11-06T23:12:00.091020Z"},"trusted":true},"outputs":[],"execution_count":null}]}